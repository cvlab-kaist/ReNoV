\begin{wraptable}{r}{0.6\textwidth}
    \vspace{-10pt}
    \centering
    {\small
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{l|ccc|ccc}
        \toprule
        \multirow{2}{*}{\centering Layer} & \multicolumn{3}{c}{PSNR $\uparrow$} & \multicolumn{3}{|c}{SSIM $\uparrow$} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7}
        & 1 view & 2 view & 3 view & 1 view & 2 view & 3 view \\
        \midrule
        Layer 4   & \textbf{15.88} & \textbf{16.19} & \textbf{16.19} & \textbf{0.567} & \textbf{0.567} & \textbf{0.568} \\
        Layer 11  & 15.54 & 15.98 & 15.99 & 0.545 & 0.550 & 0.552 \\
        Layer 17  & 14.62 & 14.53 & 14.62 & 0.541 & 0.529 & 0.526 \\
        Layer 23  & 14.40 & 14.45 & 14.48 & 0.527 & 0.510 & 0.502 \\
        \midrule
        All & 15.81 & 16.01 & 16.13 & 0.552 & 0.540 & 0.534 \\
        \bottomrule
    \end{tabular}}
    \caption{
        \textbf{Per-layer probing quantitative results.} Quantitative evaluation of MAE decoder performance using VGGT features from layers 4, 11, 17, and 23. The 4\textsuperscript{th} layer achieves optimal PSNR and SSIM scores, indicating superior encoding of both visible and occluded content for novel view synthesis. 
    }
    \vspace{-25pt}
    \label{tab:appendix_recon_quan}
    }
\end{wraptable}
