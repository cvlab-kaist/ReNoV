\section{Conclusion}

We introduce a diffusion‐based novel‐view synthesis framework that leverages VGGT’s multi‐view geometry features to unify precise reconstruction and semantically coherent inpainting. By reformulating synthesis as a warping‐and-inpainting task and injecting VGGT features into a conditioned diffusion U-Net, our method achieves state-of-the-art fidelity on both visible and occluded regions, outperforming existing diffusion-based approaches across standard benchmarks. These results underscore the value of rich geometric priors in guiding generative models, and open avenues for future extensions toward dynamic scenes and real-time applications.
%\newpage
