\section{Related work}
\label{Sec: related_work}

\paragraph{Diffusion-based 3D generation models.}
\label{Sec: nvs-not-generative}
Prior efforts in generative 3D and multi‐view synthesis have largely focused on leveraging diffusion models to bridge the gap between 2D image priors and 3D scene representations. DreamFusion~\citep{poole2022dreamfusion} first demonstrated text‐to‐3D generation by optimizing a Neural Radiance Field with a pretrained 2D diffusion prior, while ProlificDreamer~\citep{wang2023prolificdreamer} extended this paradigm by distilling multi‐view diffusion signals into a feed‐forward geometry network for faster inference. In the multi‐view setting, MVDream~\citep{shi2023mvdream} proposes a view‐consistent denoising pipeline that jointly refines color and depth across posed images, and Zero123~\citep{liu2023zero} tackles single‐image to novel‐view synthesis via a conditioned diffusion model that hallucinates plausible viewpoints. ZeroNVS~\citep{zeronvs} extends upon this method for single-view novel view synthesis, while CAT3D~\citep{gao2024cat3d} employs spatial cross-attention between generating viewpoints to achieve consistent novel view synthesis at target viewpoints. ViewCrafter~\citep{yu2024viewcrafter} delivers high-fidelity performance by finetuning a video diffusion model that conditions on point cloud representations reconstructed from the input images, enabling precise camera pose control through explicit 3D geometric priors. To handle large viewpoint changes, ViewCrafter employs an iterative view synthesis strategy with camera trajectory planning to progressively expand the point cloud coverage and synthesize novel views in previously occluded regions. While these methods have achieved impressive visual quality, they either require costly per‐scene optimization or video multi-frame generation, or rely on known camera poses, struggling with large pose extrapolation.

\paragraph{Feedforward 3D regression models.}
Feed‐forward approaches to novel‐view synthesis and 3D reconstruction bypass costly per‐scene optimization by learning rich geometric priors from large‐scale training. PixelNeRF~\citep{yu2021pixelnerf} first demonstrated how to condition a NeRF on input views via local CNN features, and IBRNet~\citep{wang2021ibrnet} built on this by fusing multi‐view depth and appearance cues in a self‐supervised stereo framework. MVSplat~\citep{chen2024mvsplat} further refines this paradigm by estimating 3D Gaussians through cost‐volume–based depth prediction, achieving high‐fidelity volumetric representations from sparse inputs. Concurrently, single‐image methods like ShapeFormer~\citep{yan2022shapeformer} exploit transformer architectures to generate novel views and coarse geometry from a one-shot image. More recently, transformer‐based systems such as DUSt3R~\citep{dust3r_cvpr24} and MASt3R~\citep{leroy2024grounding} have learned to predict point‐maps and camera poses directly from unposed images, while Noposplat~\citep{ye2024no} unifies pose estimation with 3D Gaussian fitting in a single feed‐forward pass. Likewise, FLARE~\citep{zhang2025flare} proposes a cascaded feed-forward approach that uses camera pose estimation as a bridge to guide subsequent geometry reconstruction and appearance learning for sparse-view novel view synthesis, and AnySplat~\citep{jiang2025anysplat} predicts novel view images as well as Gaussian primitives from uncalibrated image collections. However, despite their efficiency, these feed‐forward models remain fundamentally limited by reference‐view visibility, often failing to extrapolate to unseen angles or complete occluded structures without explicit inpainting or geometry completion.

\paragraph{Geometry prediction models.}  
Recent advances in geometry prediction models have enabled powerful geometric reasoning from sparse image inputs. VGGT~\citep{wang2025vggt} and DepthAnythingV3~\citep{lin2025depth} are state-of-the-art approaches capable of predicting camera parameters, depth maps, and point maps from a set of unposed images. Notably, both models build upon DINOv2~\citep{oquab2023dinov2}, inheriting its rich prior, capturing both semantic and geometric structure through self-supervised learning, yet they adopt different architectural strategies. VGGT extracts DINOv2 features as input and processes them through a separate transformer network with alternating frame attention and global attention. In contrast, Depth Anything V3 directly fine-tunes the DINOv2 model, selectively applying either frame attention or global attention at each layer. Both models then pass selected intermediate features through DPT networks~\cite{ranftl2021vision}, decoding the features into depth, point map, and camera parameters. VGGT additionally estimates point tracks for the input images, enabling higher-order geometric reasoning. While two models differ in architecture and supervision, both descend from DINOv2 and learn to reason across multiple views for geometry prediction.
In this work, we analyze how these differences affect their representations and leverage them as signals for diffusion-based novel view synthesis.