
\section{Introduction}

Novel view synthesis—predicting scene appearance from target camera viewpoints—has long been a fundamental challenge in computer vision. Recent diffusion models enable novel view generation without explicit 3D representations such as Neural Radiance Fields~\citep{mildenhall2021nerf} or 3D Gaussian Splatting~\citep{kerbl20233d}. At the same time, diffusion-based novel view synthesis, including multi-view diffusion models~\cite{watson2022novel, liu2023zero, shi2023zero123++, gao2024cat3d, szymanowicz2025bolt3d}, leverage generative priors from large-scale 2D diffusion models~\cite{rombach2022high} to synthesize novel views. These approaches map reference and noisy target images into a shared feature space, enabling the model to generate target views consistent with the reference.

% 민경 문단 2
% 문단의 목적: warping and inpainting 소개. 현존 모델은 condition으로 coordiante warping을 수행. ref 이미지가 ref/tgt의 어디에 대응되는지 알려줘서 shared space로 나타내는데 도움!
% Recent advances in diffusion-based novel view synthesis have introduced warping-and-inpainting methods~\citep{chung2023luciddreamer, seo2024genwarp} as an emerging approach. These approaches typically employ off-the-shelf geometry estimation modules—such as monocular depth predictors or DUSt3R~\citep{dust3r_cvpr24}—to first estimate camera poses and scene geometry from a reference image. The predicted geometric information guides spatial cross-attention within generation diffusion models by conditioning target viewpoint generation with warped reference coordinates~\citep{seo2024genwarp}, thereby enhancing geometric consistency between generated and the reference image.

% 민경 문단 3
% 문단의 목적: 문단1&2에서 말한 핵심1 ref간의 consistecy 유지 --> 이미 존재하는 strong visual foundation model이 잘 하고 있지 않을까? nvs에 활용하자. 
Maintaining consistency across reference views and between reference-target views is central to novel view synthesis. Recent visual foundation models~\cite{wang2025vggt, lin2025depth} trained for multi-view reasoning already possess such geometric and semantic correspondence abilities. We hypothesize that leveraging these powerful representations can serve as effective prompting signals for diffusion-based novel view synthesis. To ensure consistency between reference and target views, we draw inspiration from warping-and-inpainting novel view synthesis approaches and incorporate multi-view features into a network that projects reference features into 3D space and reprojects them onto the target viewpoint, explicitly bridging the coherence between reference and target views.

% 민경 문단 4:
% 목적: 리컨-geometry, occlusion(inpainting)-semantic, 객체 grouping (locality) 중요. 따라서 우리 분석은 feature 여러개를 분석했다.
In our analysis, we observe that novel view synthesis requires multiple capabilities: faithful reconstruction of visible regions from reference viewpoint, plausible inpainting of regions occluded in the reference images, and appropriate locality for coherent object grouping. We conduct in-depth analysis of major features~\citep{oquab2023dinov2, wang2025vggt, lin2025depth} regarding their semantic, geometric and locality awareness, as well as their novel view reconstruction capabilities from warped geometry. Our analysis reveals the geometry-enhancing capabilities of external representations, especially VGGT~\citep{wang2025vggt} and DA3~\cite{lin2025depth}, whose rich, geometrically multi-view consistent features make them suitable for novel view conditioning and generation from multiple reference images.

In this light, we introduce a novel architecture, named \textbf{Re}presentation-guided \textbf{No}vel \textbf{V}iew synthesis (shortened \textbf{ReNoV}) that leverages powerful features for novel view image prediction. We design a multi-view synthesis architecture where a reference network extracts features from multiple source views, which are then aggregated with the target-view generation features via attention in a generation network. To enhance reconstruction and inpainting performance at target viewpoint generation, we introduce projected representation prompting, a generalizable method that geometrically warps reference view external representations to the novel viewpoint, providing conditioning guidance to improve the diffusion model's synthesis quality. This approach enables our model to generate high-fidelity novel views while maintaining 3D consistency across diverse scenes and camera transformations.

Extensive experiments on RealEstate10K and zero-shot evaluation on DTU demonstrate that our method shows competitive results to state-of-the-art feedforward novel view synthesis approaches across both interpolation and extrapolation settings, with ablation studies confirming the effectiveness of our integrated semantic and geometric conditioning approach. 


% 원래 writing
% Recent advances in diffusion-based novel view synthesis have introduced warping-and-inpainting methods~\citep{chung2023luciddreamer, seo2024genwarp} as an emerging approach. These approaches typically employ off-the-shelf geometry estimation modules—such as monocular depth predictors or DUSt3R~\citep{dust3r_cvpr24}—to first estimate camera poses and scene geometry from a reference image. The predicted geometric information guides spatial cross-attention within generation diffusion models by conditioning target viewpoint generation with warped reference coordinates~\citep{seo2024genwarp}, thereby enhancing geometric consistency between generated and the reference image.

% This motivates a critical question: \textit{what qualities characterize an optimal representation for warping-and-inpainting novel view synthesis?} We observe that inpainting from partially warped information divides novel view synthesis into two distinct tasks: faithful reconstruction of visible regions that can be warped from reference viewpoint, and plausible inpainting of regions occluded in the reference image. Through empirical analysis, we find that warping-and-inpainting diffusion models~\citep{seo2024genwarp} exhibit spatial attention behaviors consistent with this insight: during reconstruction, the model seeks precise geometric correspondences with locations in the reference image, whereas during inpainting, it attends to semantically relevant features that guide generation to remain consistent with the reference view.

% This distinction suggests that representation choice significantly impacts both reconstruction quality and inpainting coherence. We conduct in-depth analysis of major features~\citep{oquab2023dinov2, weinzaepfel2022croco, wang2025vggt} regarding their semantic and geometric feature awareness, as well as their novel view reconstruction capabilities from warped geometry. Our analysis reveals the impressive capabilities of VGGT~\citep{wang2025vggt}, a recent transformer-based geometry prediction model whose rich, multiview-consistent, semantic and geometric features make it suitable for novel view conditioning and generation from multiple reference images.

% In this light, we introduce a novel architecture that leverages VGGT's powerful geometric and semantic features for novel view image prediction. We design a multi-view synthesis architecture where a reference network extracts features from multiple source views, which are then aggregated with the target-view generation features via attention in a generation network. To enhance reconstruction and inpainting performance at target viewpoint generation, we introduce a \textbf{feature warping-and-conditioning} paradigm, which geometrically warps VGGT-extracted reference features to the novel viewpoint, providing conditioning guidance to improve the diffusion model's synthesis quality, and propose \textbf{ReNoV} (\textbf{Re}presentation-guided \textbf{No}vel \textbf{V}iew synthesis). 

% Extensive experiments on RealEstate10K and zero-shot evaluation on DTU demonstrate that our method shows competitive results to state-of-the-art feedforward novel view synthesis approaches across both interpolation and extrapolation settings, with ablation studies confirming the effectiveness of our integrated semantic and geometric conditioning approach. 

