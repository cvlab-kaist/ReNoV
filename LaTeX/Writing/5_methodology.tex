\section{Method}
\subsection{Overview}
% Our objective is to predict novel view image $I_\text{tgt}$ for target viewpoint $\pi_\text{tgt}$,  combining the diffusion model's generative capabilities for consistent and realistic synthesis with external feature representations that provide strong semantic and geometric correspondence, as demonstrated in our analysis. We assume $N$ unposed and sparse reference images are given, so that $\mathcal{I_\text{ref}} = \{ I_n \in \mathbb{R}^{H \times W \times 3} \}_{n=1}^N$. Our framework, following \citep{seo2024genwarp}, is composed a reference U\textendash Net and a denoising U\textendash Net, an architecture that resembles ControlNet~\citep{zhang2023adding}. Likewise, the generative denoising network is conditioned by features extracted from reference network, which corresponds to features from reference images. The reference U\textendash Net processes the input images, geometry and VGGT feature to extract multiview features, while the denoising U\textendash Net generates the target image by refining a noisy latent, guided by warped features and geometric priors from the reference network.

Our objective is to predict a novel view image $I_\text{tgt}$ for target viewpoint $\pi_\text{tgt}$ by leveraging both the generative capabilities of diffusion models and the semantic-geometric correspondence of external feature representations validated in our analysis. Given $N$ unposed and sparse reference images $\mathcal{I_\text{ref}} = \{ I_n \in \mathbb{R}^{H \times W \times 3} \}_{n=1}^N$, we adopt a dual U-Net architecture following \citep{seo2024genwarp}, reminiscent of ControlNet~\citep{zhang2023adding}. The reference U-Net extracts multi-view features by processing input images alongside their conditioning geometric information and representations from external models~\citep{oquab2023dinov2, wang2025vggt, lin2025depth}, while the denoising U-Net synthesizes the target view through iterative refinement of a noisy latent, conditioned features from the reference network as well as geometrically warped external features of the reference images.

\subsection{Reference conditioning}
\paragraph{Geometry conditioning. }
We begin by leveraging an off-the-shelf geometry prediction model~\citep{wang2025vggt} to estimate a set of camera poses \(\{ \pi_n \in \mathbb{R}^{4 \times 4} \}_{n=1}^{N}\) and corresponding pointmaps \(\{ P_n \in \mathbb{R}^{H \times W \times 3} \}_{n=1}^{N}\), where each \(P_n\) is a 2D grid of 3D points representing the predicted world coordinates for the pixels of the reference image \(I_n\). To incorporate geometric priors into our model, we apply a positional embedding function \(\gamma(\cdot)\) to each pointmap, resulting in Fourier-encoded features \(\gamma(P_n)\), which is passed through a small pose guider network~\cite{hu2024animate} to be used as a condition for the reference and geometry prediction network.

\begin{figure*}[t]
\centering
    \includegraphics[width=\textwidth]{Figures/vggt_training_figure_new.pdf}
    % \vspace{-2em}
    \caption{\textbf{Model architecture.} (upper path) \textbf{Reference network}: \(N\) reference images are passed through an external representation model (e.g., VGGT~\cite{wang2025vggt} and DA3~\cite{lin2025depth}), producing layer--wise visual features, per--pixel point-maps, and camera pose estimates. These outputs are fused by a lightweight reference U\textendash{}Net to form a multiview reference feature.  
    The features and point-maps are then re-projected to the target camera frustum, yielding warped and aligned feature planes. (lower path) \textbf{Denoising network}: a denoising U\textendash{}Net receives a noisy latent together with the warped image and projected feature planes; at each timestep it mixes the feature from reference network with its own generation features through cross-attention, progressively refining the latent to synthesize the novel target view.}
    \label{fig:architecture}
\vspace{-5pt}
\end{figure*}


\paragraph{Representation conditioning.}
We begin by jointly processing the $N$ reference images through an external model to extract their feature representations, which are then warped to the target viewpoint to condition the denoising networkâ€”a process we term \textit{projected representation prompting}. For DINOv2, we use the final layer features for representation prompting. For VGGT and DepthAnythingV3, we extract intermediate features from transformer layers and concatenate them, following their original architectural designs. For each reference image \(I_n\), we obtain both local and global features at each selected layer, denoted as \(t_{l,n}\) and \(t_{g,n} \in \mathbb{R}^{H/P \times W/P \times 1024}\), respectively. These features are concatenated along the channel dimension to form a unified representation \(T_n = [t_{g,n}; t_{l,n}] \in \mathbb{R}^{H/P \times W/P \times 2048}\). This obtained feature is high-dimensional, exceeding what the reference U\textendash Net can efficiently process. To address this, we reduce the channel dimensionality of \(T_n\) through a convolutional network, and concatenate it with the geometric conditioning before incorporation in the initial features of reference U\textendash Nets.

% We begin by jointly processing the \(N\) reference images through an external model, extracting the representation of the reference images, then warping them to the target viewpoint to be used as condition for the target view denoising network, a process we name projected representation prompting. From DINOv2, we use the final feature of the network for the representation prompting: for both VGGT and DepthAnythingV3, we use features from the 4\textsuperscript{th}, 11\textsuperscript{th}, 17\textsuperscript{th}, and 23\textsuperscript{rd} transformer layers and concatenate them, following the architectural design of their original papers. For each reference image \(I_n\), we obtain both local and global features at each selected layer, denoted as \(t_{l,n}\) and \(t_{g,n} \in \mathbb{R}^{H/P \times W/P \times 1024}\), respectively. These features are concatenated along the channel dimension to form a unified representation \(T_n = [t_{g,n}; t_{l,n}] \in \mathbb{R}^{H/P \times W/P \times 2048}\). Yet, the obtained feature is high-dimensional, exceeding what the reference U\textendash Net can efficiently process. To address this, \(T_n\) is passed through a convolutional network. 

Analogous to the geometric conditioning, we apply a positional embedding function \(\gamma(\cdot)\) to the extracted features \(T_n\), resulting in the Fourier-encoded representation \(\gamma(T_n)\). The final reference condition \(c_n\) is obtained by concatenating the encoded image features and pointmaps:
\[
c_n = [\gamma(P_n) ; \gamma(T_n)].
\]
Following the approach of Hu et al.~\citep{hu2024animate}, each condition vector \(c_n\) is passed through a shallow convolutional network and then added to the image latents prior to input to the reference U\textendash Net.

\subsection{Projected representation prompting}
To enhance the fidelity of reconstruction and inpainting in novel view synthesis, we incorporate a geometry-driven conditioning mechanism based on warping. Specifically, we project the reference pointmaps \(\{P_1, \dots, P_N\}\) and the corresponding external representation features \(\{T_1, \dots, T_N\}\) into the target viewpoint \(\pi_{\text{tgt}}\). These projected signals provide spatial priors that guide the diffusion model toward higher-quality generation results. First, the set of reference pointmaps \(\{P_1, \dots, P_N\}\), expressed in a global coordinate frame, can be directly aggregated to form a unified point cloud:$\mathcal{P}_{\text{ref}}$. This point cloud \(\mathcal{P}_{\text{ref}} \in \mathbb{R}^{(N \times H \times W) \times 3}\) is then projected onto the target viewpoint \(\pi_{\text{tgt}}\):
\begin{equation}
  \mathcal{P}^{\Pi}_{\text{tgt}} = \Pi(\mathcal{P}_{\text{ref}}, \pi_{\text{tgt}}).
 \label{equation:projection}
\end{equation}
When multiple points are projected to the same pixel, only the one closest to the target image plane is retained, following the standard point cloud rasterization procedure~\citep{seo2024genwarp}. The resulting projected pointmap \(\mathcal{P}^{\Pi}_{\text{tgt}}\) serves as a sparse geometric condition that guides the generation of \(I_{\text{tgt}}\) from the reference views.

Given the observed multiview-consistent nature of geometric~\citep{wang2025vggt, lin2025depth} external representations, we unproject them into 3D space by anchoring each pixel-level feature to its corresponding 3D coordinate from the predicted pointmap \(P_n \in \mathbb{R}^{H \times W \times 3}\), forming a 3D feature point cloud. This pointcloud is then projected into the target view, yielding a spatially aligned warped feature map. The projected features \(T_{\text{tgt}}^{\Pi}\) and projected pointmap \(X_{\text{tgt}}^{\Pi}\) are provided as input conditions to the denoising network.

Following the same design as in the reference network, we encode \(X_{\text{tgt}}^{\Pi}\) and \(T_{\text{tgt}}^{\Pi}\) using a positional embedding function \(\mathcal{\gamma}(\cdot)\), and concatenate their Fourier embeddings with a binary visibility mask \(M_{\text{tgt}}\), which indicates grid pixels where no 3D point was projected. This forms the target correspondence condition \(c_{\text{tgt}}^d\):
\begin{equation}
c_{\text{tgt}}^d = [\mathcal{\gamma}(X_{\text{tgt}}^{\Pi}), \mathcal{\gamma}(T_{\text{tgt}}^{\Pi}), M_{\text{tgt}}].
\end{equation}
The condition \(c_{\text{tgt}}^d\) is then processed by a shallow convolutional network and added to the noise latent before being passed into the denoising U\textendash Net. As discussed in Sec.~\ref{Sec :Analysis}, providing the warped feature \(T_{\text{tgt}}^{\Pi}\) to the denoising U\textendash Net serves two key purposes: it supplies semantic priors for unseen or occluded regions by leveraging multiview-consistent features, and it delivers accurate geometric information for regions visible in the reference views. This guidance enables the model to generate more structurally faithful outputs at the target view \(\pi_{\text{tgt}}\).

\subsection{Novel-view image generation}
Following this, we conduct integrated self-and-cross attention between reference and target features, allowing the model to leverage other viewpoints, similar to \citep{seo2024genwarp}. Specifically, from the denoising U-Net, we extract key and value features of the target view, \(F_{\text{tgt}}^k, F_{\text{tgt}}^v \in \mathbb{R}^{1 \times C \times (W \times H)}\), obtained from spatial self-attention layers.  from spatial self-attention layers. These are concatenated along the viewpoint dimension with key and value features from $N$ reference views, so that the query feature $\mathbf{q} = F_{\text{tgt}}^q,$ is aggregated over attention map acquired with expanded key feature $\mathbf{k} = [F_{\text{tgt}}^k,\ F_1^k,\ \ldots,\ F_N^k]$ and value feature  $\mathbf{v} = [F_{\text{tgt}}^k,\ F_1^v,\ \ldots,\ F_N^k]$. where \(\mathbf{k}, \mathbf{v} \in \mathbb{R}^{(N+1) \times C \times (W \times H)}\). The aggregated attention is then computed as:
\begin{equation}
\text{Attention}(\mathbf{q}, \mathbf{k}, \mathbf{v}) = \text{softmax} \left( \frac{\mathbf{q}\mathbf{k}^T}{\sqrt{d_k}} \right) \mathbf{v}, \tag{6}
\end{equation}
where \(d_k\) denotes the dimensionality of the key features. Through this architecture, the generating U-Net can leverage features extracted from reference networks via attention aggregation, enabling novel view synthesis from multiple viewpoints. 

% \paragraph{Conditioning with warped feature.}


% Placeholder. Therefore, we first encode $X_t^{\Pi}$ and $T_t^{\Pi}$ using positional embedding $\mathcal{E}(\cdot)$, and concatenate the resulting Fourier feature $\mathcal{E}(X_t^{\Pi})$ and $\mathcal{E}(T_t^{\Pi})$ with a binary mask $M_t$, which marks grid pixels with no projected points. This forms the target correspondence condition $c_t^d$ for denoising network at target viewpoint $\pi_t$. Similarly, for each reference viewpoint $\pi_n$, we obtain a reference correspondence condition $c_r^r$ which consists of an embedded reference view pointmap $\mathcal{E}(X_n)$ and feature map $\mathcal{E}(T_n)$ concatenated with a one-valed tensor mask $M_n^1$
% \[
% c_t^d = [\mathcal{E}(X_t^{\Pi}),\mathcal{E}(T_t^{\Pi}), M_t], \quad
% c_n^r = [\mathcal{E}(X_n), \mathcal{E}(T_n^{\Pi}), M_n^1]. \tag{2}
% \]

% Similarily to Hu et al.~\ref{} these correspondence conditions are fist passed through a convolutional network. Then, the reference conditions are added to image latents before being fed into the reference U-Net, while the target condition is similarly added to the noise latent prior to the denoising U-Net. 

