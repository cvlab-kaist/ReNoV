\section{Motivation and Analysis}
\label{Sec :Analysis}
\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{Figures/motivation_attn_vis.pdf}
  \caption{
    \textbf{Cross-view attention maps of the denoising network~\citep{seo2024genwarp, kwak2025aligned}.} 
    A query pixel (blue dot) is chosen in the warped target view, and the resulting 
    cross-attention weights on two reference images are visualized. \textbf{Inpainting}: 
    the wheel is absent in the warped view, so attention shifts to the corresponding 
    wheels in the references. \textbf{Reconstruction}: the suitcase edge is visible, so attention 
    concentrates on the geometrically aligned edges to refine the reconstruction.
  }
  \label{fig:attention_vis}
  \vspace{-10pt}
\end{figure}

\begin{figure*}[t!]
  \centering
  \begin{subfigure}[b]{0.33\linewidth}
    \includegraphics[width=\linewidth]{Figures/geo.pdf}
    \caption{Geometric correspondence across layers}
    \label{fig:main_analysis_a}
  \end{subfigure}\hfill
  \hspace{-5px}
    \begin{subfigure}[b]{0.33\linewidth}
    \includegraphics[width=\linewidth]{Figures/sem.pdf}
    \caption{Semantic correspondence across layers} % (b)
    \label{fig:main_analysis_b}
  \end{subfigure}
  \hspace{-5px}
  \begin{subfigure}[b]{0.33\linewidth}
    \includegraphics[width=\linewidth]{Figures/lds.pdf}
    \caption{Local vs. Distant Similarity} % (c)
    \label{fig:main_analysis_c}
  \end{subfigure}
  \vspace{-5pt}
  \caption{\textbf{Analysis of visual foundation models.} (a) Geometric correspondence, (b) Semantic corrspondence \& (c) Local vs. Distant Similarity across feature layers in VGGT~\citep{wang2025vggt}, DA3-Large~\citep{lin2025depth} \& DINOv2-Large~\citep{oquab2023dinov2}. }
  \label{fig:main_analysis}
\end{figure*}

\begin{figure*}[t]
  \centering
  \vspace{-10pt}
  \includegraphics[width=\textwidth,]{Figures/geometric_corr.pdf}
  \vspace{-18pt}
  \caption{
      \textbf{Geometric correspondence.} A query point (blue dot) is selected in Frame 1, and cosine similarity maps are computed in Frame 2 and Frame 3. The scene contains featureless walls, allowing assessment of whether the model can localize the geometrically corresponding instance. Deeper layers of VGGT and DA3-L accurately identify the correct location in the corner wall aligned with the query point, while early layer 0 of VGGT and the feature of DINOv2 attend to incorrect but semantically similar locations in the wall. This illustrates that deeper layers of VGGT and DA3-L capture geometric structure more reliably than others.
  }
  \label{fig:geo_corr}
\end{figure*}

\input{Tables/recon_quan}

As discussed in Sec.~\ref{Sec: related_work}, novel view synthesis approaches fall into several categories. Non-generative approaches—e.g., MVSplat\citep{chen2024mvsplat} and NopoSplat~\citep{ye2024no}—do not exploit generative models and therefore cannot infer geometry or appearance in regions unseen or occluded in the reference images. In contrast, diffusion-based generative methods can extrapolate to viewpoints distant from the inputs; however, as these methods condition the diffusion models on target camera pose as a feature embedding, they remain confined to the pose distribution encountered during training, precluding truly arbitrary novel‐pose synthesis.


Unlike the previous methods, we interpret novel view synthesis as a warping-and-inpainting problem, akin to GenWarp~\citep{seo2024genwarp} and MoAI~\citep{kwak2025aligned}, requiring models to excel at two tasks: accurate \textit{reconstruction} of visible regions and consistent \textit{inpainting} of occluded regions. Within diffusion-based frameworks, both reconstruction and inpainting are achieved by implicitly aggregating features from reference viewpoints through the U-Net's spatial attention modules, driven by conditioning features that establish cross-view correspondences. This naturally leads to the question: what properties should an ideal conditioning feature possess for effective novel view generation?

To this end, we examine diffusion‐model attention during novel‐view synthesis and uncover a consistent pattern (Fig.~\ref{fig:attention_vis}): regions visible in the reference views—those requiring reconstruction—attend sharply to their geometric correspondences, whereas regions needing inpainting attend broadly to semantically similar locations in the references. This can be intuitively understood, as reconstruction performance hinges on pinpointing exact correspondences, while inpainting relies on semantically related context to synthesize unseen areas coherently. This motivates the search for a conditioning representation that simultaneously encodes semantic awareness and geometric correspondence. In the next section, we evaluate several representations~\citep{oquab2023dinov2, he2022masked, wang2025vggt, lin2025depth} to identify the representation that best balances semantic awareness with geometric correspondence, and offer a comprehensive analysis. To identify the optimal conditioning feature for our warping-and-inpainting diffusion framework, we compare several widely-used representations—DINOv2-L~\citep{oquab2023dinov2}, VGGT~\citep{wang2025vggt}, and DepthAnything V3-L~\citep{lin2025depth}.

\paragraph{Correspondence capability probing.}
To assess the geometric correspondence capabilities of various representations, we qualitatively and quantitatively evaluate cross-view similarity for intermediate features of each model, as shown in Fig.~\ref{fig:main_analysis} and Fig.~\ref{fig:geo_corr}. In Fig.~\ref{fig:main_analysis}, we provide per-layer quantitative results of geometric correspondence between features from multi-view images, measuring three different metrics: in Fig.~\ref{fig:main_analysis_a}, we provide geometric correspondence values, in Fig.~\ref{fig:main_analysis_b}, semantic correspondence values, and lastly, in Fig.~\ref{fig:main_analysis_c} the local vs. distant similarity (LDS) metric propsed in iREPA~\cite{singh2025matters} for measuring spatial self-similarity. In Fig.~\ref{fig:geo_corr}, we present a qualitative visualization of similarity maps for a triplet of multi-view images from a single scene, where a query point is selected in the first frame, and similarity maps are computed by comparing the first frame's features with those of the second and third frames. 

The qualitative values reveal that DINOv2~\citep{oquab2023dinov2} frequently fails to disambiguate repeated structures, revealing a lack of geometric awareness. For the VGGT~\citep{wang2025vggt} representation, we find that deeper layers (8 and on onwards) effectively capture geometric structure, attending to the correct location in the corner of the wall that is spatially aligned with the query point in subsequent frames - the quantitative results show similar results in the latter layers, showing stable and consistent geometric correspondence performance. The intermediate features of DepthAnythingV3-Large (DA3-L)~\citep{lin2025depth} exhibit progressively stronger geometric correspondence in deeper layers, reaching peak performance at layer 17. We observe that VGGT and DA3-L process multiple frames jointly and leverage their global attention mechanisms to capture geometric structure consistently across views, enabling precise localization of the corresponding object instance even in the presence of repeated or ambiguous patterns.

% --------------------------------------

\paragraph{Reconstruction capability probing.}
Building on our geometric correspondence analysis, we evaluate the intermediate features' reconstruction and inpainting capabilities for novel viewpoints through novel-view projection, examining how these correlate with their geometric correspondence metrics. To this end, we train a shallow MAE~\citep{he2022masked} decoder to predict a target view image from the warped projection of the reference view image features. The optimal feature representation should encapsulate multi-view semantic and geometric information, enabling the model to accurately reconstruct visible regions while effectively inpainting occluded areas.

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=\linewidth]{Figures/recon_qual.pdf}
    \end{center}
    \vspace{-10pt}
    \caption{
        \textbf{Qualitative results for feature analysis.}
        We warp the extracted features using point clouds,
        resulting in feature-level holes that require inpainting.
        % VGGT and DA3-L features synthesize target-view images with more accurate
        % structure and color.
    }
    \vspace{-15pt}
    \label{fig:recon_qual}
\end{figure}
For DINOv2, we directly probe the encoder output, whereas for VGGT and DA3-L, we extract intermediate features~\footnote{layer 4, 11, 17, 23 for VGGT.} \footnote{layer 11, 15, 19, 23 for DA3-L.} and use all of them in our analysis. To facilitate feature warping, we employ an off-the-shelf geometry prediction model~\citep{wang2025vggt} to obtain the pointmaps and camera poses. The token-level features are re-projected into the target view; patches without valid projections are replaced by learnable mask tokens, and training is supervised with a mean-squared-error objective.

For quantitative results, we evaluate each model for different numbers of reference views using PSNR, SSIM, and LPIPS metrics. Table~\ref{tab:recon} shows that VGGT features consistently achieve the highest results across all metrics and inference settings. In the qualitative results, Fig.~\ref{fig:recon_qual} also demonstrates that the generated images using VGGT features are most visually accurate compared to the target view images. 
% Notably, DA3-L outperforms other models in preserving geometric structures and achieving realistic pixel colors, indicating its superior effectiveness for the warping-and-inpainting approach. 